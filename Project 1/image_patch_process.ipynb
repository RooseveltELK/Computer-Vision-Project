{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b48f363-a711-43d2-94a7-6accb54b4883",
   "metadata": {},
   "source": [
    "## Image pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4ce955-1ead-4527-89a0-075868e1257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be890e-c4f3-411e-b09e-611045d9fc05",
   "metadata": {},
   "source": [
    "Set up image path and size of output image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26450917-40ed-47d9-867e-2bff97336c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_index = '01'\n",
    "fddb_annotations = './FDDB-folds/FDDB-fold-' + fold_index + '-ellipseList.txt'\n",
    "new_width = 20\n",
    "new_height = 20\n",
    "width_range = int(new_width/2)\n",
    "height_range = int(new_height/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe22ee4-1f5b-4334-803a-8b7e194317d7",
   "metadata": {},
   "source": [
    "Crop face patches by using annotations and resize it to 20x20 by using cv2.resize() method. Crop nonface patches by randomly choosing a pacth of size 20x20 outside the region of annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ec5b7d-02c1-49cb-a31d-faa88f04ce8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32428/336321712.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# read image with cv2 package and find image dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mimg_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;31m# note that the default output of cv2.imread is height, width, layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;31m# calculate bounding box of rotated ellipse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mcalc_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor_axis_radius\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mminor_axis_radius\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# ref - https://github.com/cezs/FDDB/blob/master/scripts/cs_fddb_convert_to_darknet.py\n",
    "with open(fddb_annotations, 'r') as annotations:\n",
    "    count = 0\n",
    "    for filepath in annotations:\n",
    "        # make labels/<year>/<month>/<day>/big directory tree\n",
    "        filepath_split = filepath.rstrip('\\n').rsplit('/')\n",
    "        if len(filepath_split) > 1:\n",
    "            filepath_img = './dataset/originalPics/' + '/'.join(filepath_split) + '.jpg'\n",
    "        # annotation modification to bounding box for each ellipse in image file\n",
    "        for bbox in range(int(next(annotations))):\n",
    "            # supplied values\n",
    "            current_line = next(annotations)\n",
    "            current_line_split = current_line.split()\n",
    "            major_axis_radius = float(current_line_split[0])\n",
    "            minor_axis_radius = float(current_line_split[1])\n",
    "            angle = float(current_line_split[2])\n",
    "            center_x = float(current_line_split[3])\n",
    "            center_y = float(current_line_split[4])\n",
    "            # read image with cv2 package and find image dimensions\n",
    "            image = cv2.imread(filepath_img)\n",
    "            img_height, img_width, c = image.shape # note that the default output of cv2.imread is height, width, layer\n",
    "            # calculate bounding box of rotated ellipse\n",
    "            calc_x = math.sqrt((major_axis_radius * math.cos(angle))**2 + (minor_axis_radius * math.sin(angle))**2)\n",
    "            calc_y = math.sqrt((major_axis_radius * math.sin(angle))**2 + (minor_axis_radius * math.cos(angle))**2)\n",
    "            # crop face from the original image\n",
    "            crop_image = image[max(int(center_y-calc_y),0):min(int(center_y+calc_y),img_height),max(int(center_x-calc_x),0):min(img_width,int(center_x+calc_x)),:]\n",
    "            # convert to gray image\n",
    "            crop_image = cv2.cvtColor(crop_image, cv2.COLOR_BGR2GRAY)\n",
    "            resized_img = cv2.resize(crop_image, (new_width, new_height), interpolation=cv2.INTER_LINEAR) # resize image to size of 20x20\n",
    "            # Save the cropped image\n",
    "            try:\n",
    "                os.listdir('./dataset/trimmed_img/')\n",
    "            except:\n",
    "                os.makedirs('./dataset/trimmed_img/')\n",
    "            #cv2.imwrite('./dataset/trimmed_img' + '/{}'.format(str(count) + '.jpg'), resized_img)\n",
    "            \n",
    "            \n",
    "            # crop random non-face patch from the original image\n",
    "            noface_x = np.random.randint(width_range,img_width-width_range)\n",
    "            noface_y = np.random.randint(height_range,img_height-height_range)\n",
    "            while (max(int(center_x-calc_x-width_range),0) <= noface_x <= min(img_width,int(center_x+calc_x+width_range))) & (max(int(center_y-calc_y-height_range),0) <= noface_y <= min(int(center_y+calc_y+height_range),img_height)):\n",
    "                noface_x = np.random.randint(width_range,img_width-width_range)\n",
    "                noface_y = np.random.randint(height_range,img_height-height_range)                \n",
    "            non_face_image = image[(noface_y-height_range):(noface_y+height_range),(noface_x-width_range):(noface_x+width_range),:]\n",
    "            # Save the non-face image\n",
    "            try:\n",
    "                os.listdir('./dataset/trimmed_nofaceimg/')\n",
    "            except:\n",
    "                os.makedirs('./dataset/trimmed_nofaceimg/')\n",
    "                \n",
    "            # convert to gray image\n",
    "            non_face_image = cv2.cvtColor(non_face_image, cv2.COLOR_BGR2GRAY)\n",
    "            cv2.imwrite('./dataset/trimmed_nofaceimg' + '/{}'.format(str(count) + '.jpg'), non_face_image)\n",
    "            \n",
    "            count += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257fc12f-e1e5-4fb5-927a-932bb581b594",
   "metadata": {},
   "source": [
    "Bundle the face and nonface images and shuffle it once. Split them into train dataset and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0192a6-4b68-4135-be98-c8bcc9f7052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_size = 100\n",
    "train_data_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65094763-8203-40a7-937a-53cbd562b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref - https://github.com/MarkPrecursor/SRCNN-keras/blob/master/prepare_data.py\n",
    "# Build train and test dataset\n",
    "import h5py\n",
    "\n",
    "names_face = sorted(os.listdir('./dataset/trimmed_img/'))\n",
    "names_noface = sorted(os.listdir('./dataset/trimmed_nofaceimg/'))\n",
    "\n",
    "data_face = []\n",
    "label_face = []\n",
    "\n",
    "for name in names_face:\n",
    "    fpath = './dataset/trimmed_img/' + name\n",
    "    face_img = cv2.imread(fpath)\n",
    "\n",
    "    norm_face_img = face_img.astype(np.float32) / 255. # normalize value from [0,255] to [0,1]\n",
    "\n",
    "    label_face.append(1) # label 1 means the current image contains face\n",
    "    data_face.append(norm_face_img)\n",
    "\n",
    "data_noface = []\n",
    "label_noface = []\n",
    "\n",
    "for name in names_noface:\n",
    "    fpath = './dataset/trimmed_nofaceimg/' + name\n",
    "    noface_img = cv2.imread(fpath)\n",
    "\n",
    "    norm_noface_img = noface_img.astype(np.float32) / 255. # normalize value from [0,255] to [0,1]\n",
    "\n",
    "    label_noface.append(0) # label 0 means the current image doesn't contain face\n",
    "    data_noface.append(norm_noface_img)\n",
    "    \n",
    "data_face = np.array(data_face, dtype=np.float32)\n",
    "label_face = np.array(label_face, dtype=np.int32)\n",
    "data_noface = np.array(data_noface, dtype=np.float32)\n",
    "label_noface = np.array(label_noface, dtype=np.int32)\n",
    "\n",
    "\n",
    "# shuffle the data and label to mix face images and nonface images\n",
    "index = np.arange(len(data_face))\n",
    "np.random.shuffle(index)\n",
    "data_face = data_face[index]\n",
    "label_face = label_face[index]\n",
    "\n",
    "index = np.arange(len(data_noface))\n",
    "np.random.shuffle(index)\n",
    "data_noface = data_noface[index]\n",
    "label_noface = label_noface[index]\n",
    "\n",
    "\n",
    "\n",
    "# let first train_data_size be train data\n",
    "x_train_dataset = np.concatenate((data_face[:train_data_size], data_noface[:train_data_size]), axis=0) \n",
    "y_train_dataset = np.concatenate((label_face[:train_data_size], label_noface[:train_data_size]), axis=0) \n",
    "\n",
    "x_shape = x_train_dataset.shape\n",
    "x_train_dataset = x_train_dataset.reshape((x_shape[0],x_shape[1]*x_shape[2]*x_shape[3]))\n",
    "\n",
    "# let last test_data_size be test data\n",
    "x_test_dataset = np.concatenate((data_face[-test_data_size:], data_noface[-test_data_size:]), axis=0) \n",
    "y_test_dataset = np.concatenate((label_face[-test_data_size:], label_noface[-test_data_size:]), axis=0) \n",
    "\n",
    "x_shape = x_test_dataset.shape\n",
    "x_test_dataset  = x_test_dataset.reshape((x_shape[0],x_shape[1]*x_shape[2]*x_shape[3]))\n",
    "\n",
    "# save train and test dataset\n",
    "with h5py.File('./dataset/train_dataset.h5', 'w') as h:\n",
    "    h.create_dataset('data', data=x_train_dataset, shape=x_train_dataset.shape)\n",
    "    h.create_dataset('label', data=y_train_dataset, shape=y_train_dataset.shape)\n",
    "\n",
    "with h5py.File('./dataset/test_dataset.h5', 'w') as h:\n",
    "    h.create_dataset('data', data=x_test_dataset, shape=x_test_dataset.shape)\n",
    "    h.create_dataset('label', data=y_test_dataset, shape=y_test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3c30342-9377-4c39-a9f0-c00b288613a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1200) (2000,)\n",
      "(200, 1200) (200,)\n"
     ]
    }
   ],
   "source": [
    "# verify the shape of dataset\n",
    "with h5py.File('./dataset/train_dataset.h5', 'r') as h:\n",
    "    data = np.array(h.get('data'))\n",
    "    label = np.array(h.get('label'))\n",
    "    X_train = data\n",
    "    y_train = label\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "with h5py.File('./dataset/test_dataset.h5', 'r') as h:\n",
    "    data = np.array(h.get('data'))\n",
    "    label = np.array(h.get('label'))\n",
    "    X_test = data\n",
    "    y_test = label\n",
    "\n",
    "print(X_test.shape, y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
